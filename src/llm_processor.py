# src/llm_processor.py

import sys
import os # Necess√°rio para ler a vari√°vel de ambiente da OpenAI
import requests

# --- CLIENTE E FERRAMENTAS ---
from openai import OpenAI # Cliente oficial da OpenAI
from src.database_utils import executar_query_dinamica, DynamicQuery 
from src.database_utils import get_db_connection # Necess√°rio para o main block (se for usado)


# --- 1. INICIALIZA√á√ÉO GLOBAL (AGORA USANDO OPENAI) ---
try:
    # üö® CR√çTICO: O cliente busca a chave na vari√°vel de ambiente 'OPENAI_API_KEY'
    # O valor da chave deve ser configurado no painel da Render.
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY")) 
    LLM_MODEL = 'gpt-3.5-turbo' # Modelo est√°vel e r√°pido para racioc√≠nio
except Exception as e:
    print(f"ERRO DE CLIENTE LLM: Falha ao iniciar o OpenAI Client. Verifique a chave de API. Detalhes: {e}")
    sys.exit(1)


def gerar_plano_connexa(meta_usuario: str, prazo_meses: int, membro_foco: str, categoria_foco: str):
    """
    Executa o ciclo completo: Gera√ß√£o de Feature Din√¢mica -> C√°lculo -> Gera√ß√£o de Plano pela LLM.
    """
    # 1. GERA√á√ÉO DIN√ÇMICA DE FEATURE (Busca o dado cr√≠tico no Backend)
    sql_gasto_foco = (
        f"SELECT SUM(valor) AS total_gasto_foco FROM connexa_financas "
        f"WHERE membro_id = '{membro_foco}' AND categoria = '{categoria_foco}'"
    )
    
    try:
        query_object = DynamicQuery(query=sql_gasto_foco)
        # Executando a query no Backend
        resultado_gasto = executar_query_dinamica(query_object)
        gasto_critico_total = resultado_gasto['resultado']['total_gasto_foco'] * -1
        
    except Exception as e:
        # Captura erros de DB ou de c√°lculo no Backend
        return f"Falha na busca de dados para {membro_foco} em {categoria_foco}. Erro: {e}"

    # 2. INJE√á√ÉO DE CONTEXTO E C√ÅLCULO DE METAS
    baseline_poupanca = 2703.11 
    meta_mensal_requerida = 10000 / prazo_meses 
    
    # CRIA√á√ÉO DO PROMPT MESTRE (Onde o NLP e o Dado se encontram)
    prompt_mestre = f"""
    Voc√™ √© o Consultor Financeiro Connexa, focado em planos motivacionais.

    --- CONTEXTO ANAL√çTICO ---
    Meta do Usu√°rio: {meta_usuario}. Prazo: {prazo_meses} meses.
    Poupan√ßa M√©dia Atual (Baseline): R$ {baseline_poupanca:.2f}/m√™s.
    Meta de Poupan√ßa Mensal Requerida: R$ {meta_mensal_requerida:.2f}/m√™s.
    Gasto Cr√≠tico de Foco ({membro_foco} em {categoria_foco}): R$ {gasto_critico_total:.2f} (em 18 meses).

    --- TAREFAS ---
    1. Calcule a porcentagem de corte (m√°ximo 20% e m√≠nimo 1%) que {membro_foco} deve fazer no gasto de {categoria_foco} para cobrir um d√©ficit mensal de R$ 500,00 e trace o plano.
    2. Gere a resposta em tr√™s se√ß√µes claras: "Status da Meta", "Plano de A√ß√£o Connexa" e "Dica Comportamental".
    """
    
    # 3. CHAMADA FINAL DA LLM (Onde o texto √© gerado)
    try:
        # üö® CHAMADA DA API OPENAI
        response = client.chat.completions.create(
            model=LLM_MODEL, 
            messages=[
                {"role": "system", "content": "Voc√™ √© um assistente financeiro especialista em an√°lise comportamental."},
                {"role": "user", "content": prompt_mestre}
            ]
        )
        # Retorna o texto gerado pela LLM
        return response.choices[0].message.content
        
    except Exception as e:
        # Se a chave da OpenAI n√£o for v√°lida ou o servidor falhar
        return f"\n‚ùå ERRO NA CHAMADA DA LLM: Falha ao gerar conte√∫do. Verifique sua chave da OpenAI. Detalhes: {e}"


if __name__ == "__main__":
    # Teste de Simula√ß√£o Local (A chave de API deve ser definida no terminal)
    print("\n--- IN√çCIO DO PROJETO CONNEXTA: GERA√á√ÉO DE PLANO ---")
    resultado_plano = gerar_plano_connexa(
        meta_usuario="viajar para Porto Seguro, gastando R$ 10.000", 
        prazo_meses=14,
        membro_foco="Rafael",
        categoria_foco="Lazer"
    )
    print("\n\n--- PLANO GERADO PELA IA DO CONNEXTA ---")
    print(resultado_plano)
    print("-------------------------------------------\n")